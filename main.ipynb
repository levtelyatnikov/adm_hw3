{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "import random as rm\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import heapq \n",
    "import os\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(page, index):\n",
    "    p = requests.get(str(page))\n",
    "    with open( str(index) + \".html\", \"w\" ) as fh:\n",
    "        fh.write(p.text)\n",
    "        \n",
    "#path = \"file:///Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_html/article_9999.html\"\n",
    "def re_fr_dir(path):\n",
    "    page = urlopen(path)\n",
    "    movie_soup = BeautifulSoup(page.read().decode(\"utf-8\"), 'html.parser')\n",
    "    return movie_soup\n",
    "#movie_soup = re_fr_dir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon\n",
      "https://en.wikipedia.org/wiki/The_Martyred_Presidents\n",
      "https://en.wikipedia.org/wiki/Terrible_Teddy,_the_Grizzly_King\n",
      "https://en.wikipedia.org/wiki/Jack_and_the_Beanstalk_(1902_film)\n",
      "https://en.wikipedia.org/wiki/Alice_in_Wonderland_(1903_film)\n"
     ]
    }
   ],
   "source": [
    "movies_url = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html\"\n",
    "movies_response = requests.get(movies_url)\n",
    "print(movies_response)\n",
    "movies_soup = BeautifulSoup(movies_response.text, 'html.parser')\n",
    "lst_movies = movies_soup.select(\"a\")\n",
    "lst_urls = []\n",
    "for movie in range(0,len(lst_movies)):\n",
    "    lst_urls.append(lst_movies[movie].get(\"href\")) # get all links of the movies\n",
    "print('\\n'.join(lst_urls[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i need to comment it because i dont want to wait again\n",
    "# for i in range(len(lst_urls)) :\n",
    "#     save_html(lst_urls[i],\"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_html/\" + \"article_\" + str(i))\n",
    "#     if i%10 == 0:\n",
    "#         time.sleep(int(rm.uniform(1, 5)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all path to the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pdir = '/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_html/'\n",
    "movies = []\n",
    "for d, dirs, files in os.walk(pdir):\n",
    "    for f in files:\n",
    "        movies.append(\"file:///Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_html/\" + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the plot\n",
    "def extract_plot_artc(m_soup):\n",
    "    article = ''\n",
    "    plot = ''\n",
    "    pl_soup = m_soup.select(\".mw-parser-output > p, .mw-parser-output > h2\") # select the class and tags where is plot and intro\n",
    "    seen_tags = []\n",
    "    flag = 0\n",
    "    for pl in pl_soup:\n",
    "        seen_tags.append(pl.name)\n",
    "        if seen_tags.count(\"h2\")==0:  # count the tags\n",
    "            article = article + pl.text # if it is before tag h2 it is intro\n",
    "        if seen_tags.count(\"h2\")==1 and flag == 1:\n",
    "            plot = plot + pl.text # if it is after first tag h2 it is plot\n",
    "        if seen_tags.count(\"h2\")==1:\n",
    "            flag = 1\n",
    "        if seen_tags.count(\"h2\")==2:\n",
    "            break  \n",
    "    if article == '': # if there is not intro => it is NA\n",
    "        article = \"NA\"\n",
    "    if plot == '': # if there is not plot => it is NA\n",
    "        plot == 'NA'\n",
    "    return article, plot\n",
    "\n",
    "def get_title(m_soup):\n",
    "    title = re.findall(r\".+?(?=\\s\\D\\sWikipedia$)\",m_soup.find(\"title\").text)[0] # extract the title and drop the name - Wikipedia\n",
    "    if title == \"\": # if there is not title => it is NA\n",
    "        return \"NA\"\n",
    "    else:\n",
    "        return title\n",
    "# extract the information in the table\n",
    "def extract_ch(row):\n",
    "    res = []\n",
    "    try:\n",
    "        for ele in row.find(\"td\").children: # some part of table describet from the tag p\n",
    "            res.append(ele.text)\n",
    "    except: \n",
    "        try:\n",
    "            for ele in row.find(\"td\"): # some part of table describet from the tag br which has class 'bs4.element.NavigableString'\n",
    "                if 'bs4.element.NavigableString' in str(type(ele)):\n",
    "                    res.append(ele)\n",
    "        except:\n",
    "            pass\n",
    "    return \" \".join(res)\n",
    "# this function account the information\n",
    "def info(m_soup,d):\n",
    "    line = m_soup.find(\"table\",{\"class\": \"infobox vevent\"}).find('tbody').find_all('tr')\n",
    "    d[\"title\"] = get_title(movie_soup) # title\n",
    "    d[\"intro\"],d[\"plot\"] = extract_plot_artc(m_soup)\n",
    "    flag_page_title = 0\n",
    "    for row in line: # take the row of table\n",
    "        try:\n",
    "            if flag_page_title == 0:\n",
    "                d[\"film_name\"]  = row.find(\"th\").text\n",
    "                flag_page_title = 1 # we need this flag to get the name of film\n",
    "            if row.find(\"th\").text in d.keys():\n",
    "                d[row.find(\"th\").text] = extract_ch(row) #extract the table info\n",
    "        except:\n",
    "            pass\n",
    "    return d\n",
    "# remove the stop words and stemming and splitting\n",
    "def remov_stop_w_and_p_stemm(s):\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    tok = RegexpTokenizer(r\"\\w+\")\n",
    "    dirty_words = tok.tokenize(s) # split text\n",
    "    porter = PorterStemmer()\n",
    "    stem_words = list(map(porter.stem,dirty_words)) #stemm words\n",
    "    words = filter(lambda x: x not in string.punctuation, stem_words) # drop punctuation\n",
    "    cleaned_text = filter(lambda x: x not in stop_words, words) # drop stop words\n",
    "    return \" \".join(list(cleaned_text)) # create the text again\n",
    "\n",
    "# prepare the dict for scrapping all informations\n",
    "d = {\"title\":\"\",\"intro\":\"\",\"plot\":\"\",\"film_name\" : \"\",\"Directed by\" : \"\",\"Produced by\" : \"\",\"Written by\" : \"\",\"Starring\" : \"\" ,\"Music by\" : \"\", \n",
    "     \"Release date\" : \"\",\"Running time\" : \"\",\"Country\": \"\",\"Language\" : \"\",\"Budget\" : \"\",\"clean_intro\":\"\",\n",
    "    \"clean_plot\":\"\",\"url\":\"\"}\n",
    "for key,value in d.items():\n",
    "    if d[key] == \"\":\n",
    "        d[key] = \"NA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the info from pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## vocab = \"\" # during the extracting information i want to create the vocabulary\n",
    "# for path in movies:\n",
    "#     lost_pages = []\n",
    "#     try:\n",
    "#         movie_soup = re_fr_dir(path) # take the movie\n",
    "#         doc = info(movie_soup,d) # take information\n",
    "#     except:\n",
    "#         lost_pages.append(path) # catch the expectations\n",
    "#     for key,value in doc.items():\n",
    "#         doc[key] = re.sub(r'\\n', r\"\",doc[key]) # from every info block i want to delete the \\n and \\t\n",
    "#         doc[key] = re.sub(r'\\t', r\"\",doc[key]) # from every info block i want to delete the \\n and \\t\n",
    "#     doc[\"Release date\"] = unicodedata.normalize(\"NFKD\", doc[\"Release date\"]) # normilize the time\n",
    "#     doc[\"clean_intro\"] = remov_stop_w_and_p_stemm(doc[\"intro\"]) # create the clean intro and plot\n",
    "#     doc[\"clean_plot\"] = remov_stop_w_and_p_stemm(doc[\"plot\"])\n",
    "##     vocab = vocab + \" \" + doc[\"clean_intro\"] + \" \" + doc[\"clean_plot\"] # creating the vocabulary\n",
    "#     doc[\"url\"] = str(lst_urls[int(re.findall(r'\\d+',re.findall(r'article_.+',path)[0])[0])])\n",
    "#     file_name = \"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/\" + re.findall(r'article_.+\\.',path)[0] + \"tsv\"\n",
    "#     # save the tsv file\n",
    "#     with open(file_name, 'w', newline='') as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames = list(doc.keys()), dialect = \"excel-tab\")\n",
    "#         writer.writeheader()\n",
    "#         f.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\" % tuple(doc.values()))\n",
    "        \n",
    "    \n",
    "    \n",
    "# # #     with open(file_name, \"w\",encoding=\"utf-8\") as f: \n",
    "# # #         f.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\" % tuple(doc.values()))\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdir = '/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/'\n",
    "movies_tsv = []\n",
    "for d, dirs, files in os.walk(pdir):\n",
    "    for f in files:\n",
    "        movies_tsv.append(\"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/\" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(data):\n",
    "    s1 = ''\n",
    "    s2 = ''\n",
    "    if data.clean_intro.isna()[0]: # sometimes there is not plot so we need to prevent it \n",
    "        s1 = ''\n",
    "    else:\n",
    "        s1 = data.clean_intro.str[:][0]\n",
    "        \n",
    "    if data.clean_plot.isna()[0]:\n",
    "        s2 = ''\n",
    "    else:\n",
    "        s2 = data.clean_plot.str[:][0]\n",
    "    return s1 + \" \" + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = \"\"\n",
    "# miss_tsv = []\n",
    "# for path in movies_tsv:\n",
    "#     #try:\n",
    "#     df = pd.read_csv(path, sep='\\t', index_col= False,encoding='utf-8')\n",
    "#     try:\n",
    "#         vocab = vocab + \" \" + prepare_text(df)\n",
    "#     except:\n",
    "#         miss_tsv.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once i need to create and save the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = RegexpTokenizer(r\"\\w+\")\n",
    "# tru_vocab = set(tok.tokenize(vocab)) # i splited all intro and plot with this function so i have to split all vocab with this vunction\n",
    "# tru_vocab = (list(tru_vocab))\n",
    "# vocabulary = {}\n",
    "# for i in range(len((tru_vocab))):\n",
    "#     vocabulary[i] = tru_vocab[i]\n",
    "    \n",
    "\n",
    "# with open('vocabulary.txt','w') as out:\n",
    "#     for key,val in vocabulary.items():\n",
    "#         out.write('{}:{}\\n'.format(key,val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "with open('vocabulary.txt') as file:\n",
    "    for line in file:\n",
    "        value,key = line.split(\":\")\n",
    "        vocabulary[key.strip(\"\\n\")] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_doc = {} # prepare the firs invert index \n",
    "# for i in range(len(vocabulary.values())):\n",
    "#     term_doc[i] = []\n",
    "\n",
    "# for path in movies_tsv:\n",
    "#     number = re.findall(r'\\d+',path)[1] #number of document\n",
    "#     df = pd.read_csv(path, sep='\\t', index_col= False,encoding='utf-8') # open the doc\n",
    "#     # take the plot and intro\n",
    "#     try:\n",
    "#         clean_intro = df.clean_intro.str[:][0]\n",
    "#     except:\n",
    "#         clean_intro = ''\n",
    "#     try:\n",
    "#         clean_plot = df.clean_plot.str[:][0]\n",
    "#     except:\n",
    "#         clean_plot = ''\n",
    "#     tok = RegexpTokenizer(r\"\\w+\") \n",
    "#     content = list(set(tok.tokenize((clean_intro + \" \" + clean_plot)))) # take the set of words in document \n",
    "    \n",
    "\n",
    "#     for word in content:\n",
    "#         term_doc[int(vocabulary[word])].append(int(number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Inverted_index.json', 'w') as fp:\n",
    "#     json.dump(term_doc, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x\n",
    "Inv_in = json.load(open('Inverted_index.json')) # as we can see now all key are str\n",
    "\n",
    "Inv_in = jsonKeys2int(Inv_in) # this finction solved the problem with string key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_upload():\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tok = RegexpTokenizer(r\"\\w+\")\n",
    "    porter = PorterStemmer()\n",
    "    query = tok.tokenize(input()) # take the query and we need to split and stemm it as we sptlited vocab\n",
    "    query_words = list(map(porter.stem,query)) \n",
    "    words = filter(lambda x: x not in string.punctuation, query_words)\n",
    "    cleaned_query = filter(lambda x: x not in stop_words, words)\n",
    "    return cleaned_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film studio america first motion pictur industri base begin 20th centuri\n"
     ]
    }
   ],
   "source": [
    "query = list(query_upload())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ex try this : film studio america first motion pictur industri base begin 20th centuri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_query(words,vocabulary):\n",
    "    res = []\n",
    "    for i in range(len(words)):\n",
    "        res.append(int(vocabulary[words[i]]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_query = term_query(query,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the documents which contains all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_search_engine(paths):\n",
    "    if len(paths) > 1:\n",
    "        return pd.concat([pd.read_csv(path,sep = \"\\t\", index_col = False,encoding='utf-8')[[\"title\",\"intro\",\"url\"]] for path in paths]).reset_index(drop = True)\n",
    "    else:\n",
    "        return pd.concat([pd.read_csv(path,sep = \"\\t\", index_col = False,encoding='utf-8')[[\"title\",\"intro\",\"url\"]] for path in paths[:1]]).reset_index(drop = True)\n",
    "    return res\n",
    "def Seqrch_Engine1(query,inver_in):\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        docs.append(inver_in[term])\n",
    "    for i in range(len(docs)): # from list to sets\n",
    "        docs[i] = set(docs[i])\n",
    "    res  = docs[0].intersection(*[i for i in docs[:]])\n",
    "    if len(res) != 0 :\n",
    "        paths = []\n",
    "        for i in res:\n",
    "            paths.append(\"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/article_\" + str(i) + \".tsv\")\n",
    "        output = output_search_engine(paths)\n",
    "        \n",
    "        return output_search_engine(paths)\n",
    "            \n",
    "    else:\n",
    "        return print(\"There is not any movie with ALL whis words\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Musketeers of Pig Alley</td>\n",
       "      <td>The Musketeers of Pig Alley is a 1912 American...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Musketeers_o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Black Viper</td>\n",
       "      <td>The Black Viper (aka La vipère noire in France...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Black_Viper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Unseen Enemy</td>\n",
       "      <td>An Unseen Enemy is a 1912 Biograph Company sho...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/An_Unseen_Enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Call of the Wild (1908 film)</td>\n",
       "      <td>The Call of the Wild is a 1908 American short ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Call_of_the_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Poor Little Rich Girl</td>\n",
       "      <td>The Poor Little Rich Girl is a 1917 American c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Poor_Little_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>At the Altar</td>\n",
       "      <td>At the Altar is a 1909 American silent drama f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/At_the_Altar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Venus Model</td>\n",
       "      <td>The Venus Model is a 1918 American silent roma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Venus_Model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0       The Musketeers of Pig Alley   \n",
       "1                   The Black Viper   \n",
       "2                   An Unseen Enemy   \n",
       "3  The Call of the Wild (1908 film)   \n",
       "4         The Poor Little Rich Girl   \n",
       "5                      At the Altar   \n",
       "6                   The Venus Model   \n",
       "\n",
       "                                               intro  \\\n",
       "0  The Musketeers of Pig Alley is a 1912 American...   \n",
       "1  The Black Viper (aka La vipère noire in France...   \n",
       "2  An Unseen Enemy is a 1912 Biograph Company sho...   \n",
       "3  The Call of the Wild is a 1908 American short ...   \n",
       "4  The Poor Little Rich Girl is a 1917 American c...   \n",
       "5  At the Altar is a 1909 American silent drama f...   \n",
       "6  The Venus Model is a 1918 American silent roma...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://en.wikipedia.org/wiki/The_Musketeers_o...  \n",
       "1      https://en.wikipedia.org/wiki/The_Black_Viper  \n",
       "2      https://en.wikipedia.org/wiki/An_Unseen_Enemy  \n",
       "3  https://en.wikipedia.org/wiki/The_Call_of_the_...  \n",
       "4  https://en.wikipedia.org/wiki/The_Poor_Little_...  \n",
       "5         https://en.wikipedia.org/wiki/At_the_Altar  \n",
       "6      https://en.wikipedia.org/wiki/The_Venus_Model  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seqrch_Engine1(t_query,Inv_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "number_of_documents = len(movies_tsv) # the number of document in which we will search the movie\n",
    "def idf(Inv_in,number_of_documents):\n",
    "    term_idf = {}\n",
    "    # idf = log2(number of document / number of doc consists the term key )\n",
    "    lost_words = []\n",
    "    for key,values in Inv_in.items(): \n",
    "        try:\n",
    "            term_idf[key] = math.log(number_of_documents/len(values),10) #logarith by 10\n",
    "        except:\n",
    "            lost_words.append(key)\n",
    "    return lost_words,term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_words,term_idf = idf(Inv_in,number_of_documents) # create the idf for every file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s):\n",
    "    tok = RegexpTokenizer(r\"\\w+\")\n",
    "    words = tok.tokenize(s)\n",
    "    return words\n",
    "\n",
    "def find_word (vocab,word_id):\n",
    "    for word,term in vocab.items():\n",
    "        if int(term) == word_id:\n",
    "            break \n",
    "    return word\n",
    "def prepare_text(data):\n",
    "    s1 = ''\n",
    "    s2 = ''\n",
    "    if data.clean_intro.isna()[0]: # sometimes there is not plot so we need to prevent it \n",
    "        s1 = ''\n",
    "    else:\n",
    "        s1 = data.clean_intro.str[:][0]\n",
    "        \n",
    "    if data.clean_plot.isna()[0]:\n",
    "        s2 = ''\n",
    "    else:\n",
    "        s2 = data.clean_plot.str[:][0]\n",
    "        \n",
    "    return s1 + \" \" + s2\n",
    "def word_to_term(lst_words , vocab):\n",
    "    terms = []\n",
    "    for word in lst_words:\n",
    "        try:\n",
    "            terms.append(int(vocab[word]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return terms \n",
    "\n",
    "def count_length(terms,idf_ls,len_document):\n",
    "    lenght = 0\n",
    "    for term in terms:\n",
    "        lenght += (idf_ls[term]/len_document)**2\n",
    "    \n",
    "    return np.sqrt(lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Inverted Index 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_ind2 = {}\n",
    "# doc_lenght = {}\n",
    "# for key,value in Inv_in.items():\n",
    "#     inv_ind2[key] = []\n",
    "\n",
    "\n",
    "# for path in movies_tsv:\n",
    "#     doc_num = int(re.findall(r'\\d+',re.findall(r'article_.+',path)[0])[0])\n",
    "#     df = pd.read_csv(path,sep = \"\\t\", index_col = False,encoding='utf-8')[[\"clean_intro\",\"clean_plot\"]] # open the file with DataFrame\n",
    "#     text = prepare_text(df) # prepare the text to avoid any NaN\n",
    "#     words = split_text(text)\n",
    "#     terms = word_to_term(words , vocabulary)\n",
    "#     doc_lenght[doc_num] = count_length(terms,term_idf,len(terms))\n",
    "#     d_length = len(terms)\n",
    "#     unic_terms = list(set(terms))\n",
    "\n",
    "#     for t in unic_terms:    \n",
    "#         freq = terms.count(t)\n",
    "#         td_idf = freq * term_idf[t] # idf of term multiply on td\n",
    "#         inv_ind2[t].append((doc_num, td_idf/d_length))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #as we can understand the length of inverted index have to be tha same so as we can see there are the same it means that everithing is okey\n",
    "# a = []\n",
    "# for key1,value1 in inv_ind2.items():\n",
    "#     if len(Inv_in[key1]) == len(value1):\n",
    "#          pass\n",
    "#     else:\n",
    "#         a.append(key1)\n",
    "    \n",
    "# import json\n",
    "# with open('Inverted_index_2_1.json', 'w') as fp:\n",
    "#     json.dump(inv_ind2, fp)\n",
    "    \n",
    "# with open('doc_lenght.json', 'w') as fp:\n",
    "#     json.dump(doc_lenght, fp)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save inverted index 2.1 and length of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Inverted_index_2_1.json', 'w') as fp:\n",
    "#     json.dump(inv_ind2, fp)\n",
    "    \n",
    "# with open('doc_lenght.json', 'w') as fp:\n",
    "#     json.dump(doc_lenght, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload inverted index 2.1 and length of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind2 = json.load(open('Inverted_index_2_1.json')) # as we can see now all key are str\n",
    "inv_ind2 = jsonKeys2int(inv_ind2) # this finction solved the problem with string key\n",
    "for key in inv_ind2.keys(): # after uploading json all tuples became lists \n",
    "    for i in range(len(inv_ind2[key])):\n",
    "        inv_ind2[key][i] = tuple(inv_ind2[key][i]) # turn back from lists to tuples\n",
    "for key in inv_ind2.keys():\n",
    "    for i in range(len(inv_ind2[key])):\n",
    "        inv_ind2[key][i] = tuple(inv_ind2[key][i])\n",
    "doc_lenght = json.load(open('doc_lenght.json')) # as we can see now all key are str\n",
    "doc_lenght = jsonKeys2int(doc_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiery_tdidf(q,idf):\n",
    "    q_td_idf = {}\n",
    "    # idf = log2(number of document / number of doc consists the term key )\n",
    "    for term in q:\n",
    "        q_td_idf[term] = idf[term]/len(q) # quiery tdidf (td = 1/len(document))\n",
    "    return q_td_idf \n",
    "\n",
    "def Search_engine_2_all_docs(query,vocabulary,idf,inv_in):\n",
    "    t_query = term_query(query,vocabulary) # take the words and create the string\n",
    "    lenght_q = count_length(t_query,idf,len(t_query))\n",
    "    q_td_idf = quiery_tdidf(t_query,term_idf)\n",
    "    terms_in_docs = []\n",
    "    cossim = {}\n",
    "    d = {}\n",
    "    for term in t_query:\n",
    "        terms_in_docs.append((term,inv_in[term]))\n",
    "    \n",
    "    for info in terms_in_docs:\n",
    "        term = info[0]\n",
    "        for doc_tdidf in info[1]:\n",
    "            if doc_tdidf[0] in d.keys(): \n",
    "                d[doc_tdidf[0]].append(doc_tdidf[1] * q_td_idf[term])\n",
    "            elif doc_tdidf[0] not in d.keys():\n",
    "                d[doc_tdidf[0]] = []\n",
    "                d[doc_tdidf[0]].append(doc_tdidf[1] * q_td_idf[term])\n",
    "        for num_doc in d.keys():\n",
    "            cossim[str(num_doc)] = sum(d[num_doc])/(doc_lenght[num_doc]*lenght_q ) # count the cosSim\n",
    "\n",
    "    return cossim\n",
    "\n",
    "# this function works like first search engine but the output is the list of documents which conteins all words from the quaery\n",
    "def find_all_docs_consist_all_quiery(query,inver_in):\n",
    "    docs = []\n",
    "    for term in query:\n",
    "        docs.append(inver_in[term])\n",
    "    for i in range(len(docs)): # from list to sets\n",
    "        docs[i] = set(docs[i])\n",
    "    res  = docs[0].intersection(*[i for i in docs[:]])\n",
    "    if len(res) != 0 :\n",
    "        paths = []\n",
    "        for i in res:\n",
    "            paths.append(\"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/article_\" + str(i) + \".tsv\")\n",
    "        return paths\n",
    "            \n",
    "    else:\n",
    "        return [] # if there if not such documents\n",
    "    return res\n",
    "# Search engine 2\n",
    "def Search_engine_2(query,vocabulary,idf,inv_in,paths_doc):\n",
    "    terms_in_docs = []\n",
    "    cossim = {}\n",
    "    d = {}\n",
    "    for key in paths_doc:\n",
    "        d[int(re.findall(r'\\d+',re.findall(r'article_.+',key)[0])[0])] = [] #prepare the dict of the documents which contains all words from the query\n",
    "    # prepare the query    \n",
    "    t_query = term_query(query,vocabulary) # take the words and create the string\n",
    "    lenght_q = count_length(t_query,idf,len(t_query)) # count the norm of the query\n",
    "    q_td_idf = quiery_tdidf(t_query,term_idf) # return tdidf query\n",
    "    #prepare\n",
    "    for term in t_query:\n",
    "        terms_in_docs.append((term,inv_in[term])) #take the inverted index only for that terms which is in the query\n",
    "    # take the term\n",
    "    for info in terms_in_docs:\n",
    "        term = info[0] # take the term\n",
    "        for doc_tdidf in info[1]: #take the number of document\n",
    "            if doc_tdidf[0] in d.keys(): # if the number of doc in the prepared list of documents which consists all words from the query\n",
    "                d[doc_tdidf[0]].append(doc_tdidf[1] * q_td_idf[term]) # count the numerator cosine similarity\n",
    "\n",
    "    for num_doc in d.keys():\n",
    "        cossim[str(num_doc)] = sum(d[num_doc])/(doc_lenght[num_doc]*lenght_q ) # count the cosine simimilarity\n",
    "\n",
    "    return cossim\n",
    "# this function printing creating the top k movies\n",
    "def output_search_engine(paths,top_k):\n",
    "    if len(paths) > 1:\n",
    "        a = pd.concat([pd.read_csv(path,sep = \"\\t\", index_col = False)[[\"title\",\"intro\",\"url\"]] for path in paths]).reset_index(drop = True)\n",
    "        a['Similarity'] = top_k\n",
    "        return a\n",
    "    else:\n",
    "        a = pd.concat([pd.read_csv(path,sep = \"\\t\", index_col = False)[[\"title\",\"intro\",\"url\"]] for path in paths]).reset_index(drop = True)\n",
    "        a['Similarity'] = top_k\n",
    "        return a\n",
    "# this function use the HEAP structure to find the top k movies\n",
    "def top_k(k,cossim):\n",
    "    heap = [(value, key) for key,value in cossim.items()] # to do a heap from the dict we need as key the score of movies\n",
    "    largest = heapq.nlargest(k, heap) #get top k movies \n",
    "    paths = []\n",
    "    # take the paths of top k movies\n",
    "    paths = [\"/Users/macbook/Desktop/Lev/Sapienza/ADM_Labs/HW3/data_tsv/article_\" + str(num_doc[1]) + \".tsv\" for num_doc in largest]\n",
    "    # print(paths)\n",
    "    return output_search_engine(paths,[i for i,j in largest]) # print the top k movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star future space\n"
     ]
    }
   ],
   "source": [
    "query = list(query_upload()) #take the string and create the words\n",
    "t_query = term_query(query,vocabulary)\n",
    "paths_doc = find_all_docs_consist_all_quiery(t_query,Inv_in) # i use first search engine to find all documents which consist all quaery\n",
    "if len(paths_doc) == 0: # if there is not documents which contains all words from the query\n",
    "    cossim = Search_engine_2_all_docs(query,vocabulary,term_idf,inv_ind2) # this function will count the cossine similarity for all documents which consists at least one wod from the query\n",
    "else:\n",
    "    cossim = Search_engine_2(query,vocabulary,term_idf,inv_ind2,paths_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example query: star future space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print top k = 5 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Jetsons Meet the Flintstones</td>\n",
       "      <td>The Jetsons Meet the Flintstones is a 1987 ani...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Jetsons_Meet...</td>\n",
       "      <td>0.588534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost in Space (film)</td>\n",
       "      <td>Lost in Space is a 1998 American science-fict...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lost_in_Space_(f...</td>\n",
       "      <td>0.303305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zathura: A Space Adventure</td>\n",
       "      <td>Zathura: A Space Adventure (also known simply...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zathura_(film)</td>\n",
       "      <td>0.267976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Phantom Planet</td>\n",
       "      <td>The Phantom Planet is a 1961 independently mad...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Phantom_Planet</td>\n",
       "      <td>0.254209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predestination (film)</td>\n",
       "      <td>Predestination is a 2014 Australian science f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Predestination_(...</td>\n",
       "      <td>0.195048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0  The Jetsons Meet the Flintstones   \n",
       "1              Lost in Space (film)   \n",
       "2        Zathura: A Space Adventure   \n",
       "3                The Phantom Planet   \n",
       "4             Predestination (film)   \n",
       "\n",
       "                                               intro  \\\n",
       "0  The Jetsons Meet the Flintstones is a 1987 ani...   \n",
       "1   Lost in Space is a 1998 American science-fict...   \n",
       "2   Zathura: A Space Adventure (also known simply...   \n",
       "3  The Phantom Planet is a 1961 independently mad...   \n",
       "4   Predestination is a 2014 Australian science f...   \n",
       "\n",
       "                                                 url  Similarity  \n",
       "0  https://en.wikipedia.org/wiki/The_Jetsons_Meet...    0.588534  \n",
       "1  https://en.wikipedia.org/wiki/Lost_in_Space_(f...    0.303305  \n",
       "2       https://en.wikipedia.org/wiki/Zathura_(film)    0.267976  \n",
       "3   https://en.wikipedia.org/wiki/The_Phantom_Planet    0.254209  \n",
       "4  https://en.wikipedia.org/wiki/Predestination_(...    0.195048  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "top_k(k,cossim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use top 5 because i suppose that 5 option of the movies is enough to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Search engine 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idia is to add the title to the vocabulary. Also i want to use a little bit different tdidf weight scheme,\n",
    "I decided to improve the idf. The formula for search engine 2 idf = log(N/N_t) which means that if in every document we have one tearm we will get log(1) = 0, so to prevent it i decided to use formula idf = log(1 + N/N_t) which means that the lower bound of the terms will be log(2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ti_text(data):\n",
    "    s1 = ''\n",
    "    s2 = ''\n",
    "    s3 = ''\n",
    "    if data.clean_intro.isna()[0]: # sometimes there is not plot so we need to prevent it \n",
    "        s1 = ''\n",
    "    else:\n",
    "        s1 = data.clean_intro.str[:][0]\n",
    "    if data.clean_plot.isna()[0]:\n",
    "        s2 = ''\n",
    "    else:\n",
    "        s2 = data.clean_plot.str[:][0]\n",
    "    if data.title.isna()[0]: \n",
    "        s3 = ''\n",
    "    else:\n",
    "        s3 = data.title.str[:][0]\n",
    "    return s1 + \" \" + s2 + \" \" + s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = \"\"\n",
    "# miss_tsv = []\n",
    "# for path in movies_tsv:\n",
    "#     #try:\n",
    "#     df = pd.read_csv(path, sep='\\t', index_col= False,encoding='utf-8')\n",
    "#     try:\n",
    "#         vocab = vocab + \" \" + prepare_text(df)\n",
    "#     except:\n",
    "#         miss_tsv.append(path)\n",
    "# tok = RegexpTokenizer(r\"\\w+\")\n",
    "# tru_vocab = set(tok.tokenize(vocab)) # i splited all intro and plot with this function so i have to split all vocab with this vunction\n",
    "# tru_vocab = (list(tru_vocab))\n",
    "# vocabulary = {}\n",
    "# for i in range(len((tru_vocab))):\n",
    "#     vocabulary[i] = tru_vocab[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocabulary_search3.txt','w') as out:\n",
    "#     for key,val in vocabulary.items():\n",
    "#         out.write('{}:{}\\n'.format(key,val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the inverted vocabular with the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "with open('vocabulary_search3.txt') as file:\n",
    "    for line in file:\n",
    "        value,key = line.split(\":\")\n",
    "        vocabulary[key.strip(\"\\n\")] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_doc = {} # prepare the firs invert index \n",
    "# for i in range(len(vocabulary.values())):\n",
    "#     term_doc[i] = []\n",
    "\n",
    "# for path in movies_tsv:\n",
    "#     number = re.findall(r'\\d+',path)[1] #number of document\n",
    "#     df = pd.read_csv(path, sep='\\t', index_col= False,encoding='utf-8') # open the doc\n",
    "#     # take the plot and intro\n",
    "#     try:\n",
    "#         clean_intro = df.clean_intro.str[:][0]\n",
    "#     except:\n",
    "#         clean_intro = ''\n",
    "#     try:\n",
    "#         clean_plot = df.clean_plot.str[:][0]\n",
    "#     except:\n",
    "#         clean_plot = ''\n",
    "#     tok = RegexpTokenizer(r\"\\w+\") \n",
    "#     content = list(set(tok.tokenize((clean_intro + \" \" + clean_plot)))) # take the set of words in document \n",
    "    \n",
    "\n",
    "#     for word in content:\n",
    "#         term_doc[int(vocabulary[word])].append(int(number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Inverted_index_3.json', 'w') as fp:\n",
    "#     json.dump(term_doc, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x\n",
    "Inv_in = json.load(open('Inverted_index_3.json')) # as we can see now all key are str\n",
    "\n",
    "Inv_in = jsonKeys2int(Inv_in) # this finction solved the problem with string key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = len(movies_tsv) # the number of document in which we will search the movie\n",
    "def idf(Inv_in,number_of_documents):\n",
    "    term_idf = {}\n",
    "    # idf = log2(number of document / number of doc consists the term key )\n",
    "    lost_words = []\n",
    "    for key,values in Inv_in.items(): \n",
    "        try:\n",
    "            term_idf[key] = math.log(1 + (number_of_documents/len(values)),10) # smoothing the tdidf score\n",
    "        except:\n",
    "            lost_words.append(key)\n",
    "    return lost_words,term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_words,term_idf = idf(Inv_in,number_of_documents) # create the idf for every file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ti_text(data):\n",
    "    s1 = ''\n",
    "    s2 = ''\n",
    "    s3 = ''\n",
    "    if data.clean_intro.isna()[0]: # sometimes there is not plot so we need to prevent it \n",
    "        s1 = ''\n",
    "    else:\n",
    "        s1 = data.clean_intro.str[:][0]\n",
    "    if data.clean_plot.isna()[0]:\n",
    "        s2 = ''\n",
    "    else:\n",
    "        s2 = data.clean_plot.str[:][0]\n",
    "    if data.title.isna()[0]: \n",
    "        s3 = ''\n",
    "    else:\n",
    "        s3 = data.title.str[:][0]\n",
    "    return s1 + \" \" + s2 + \" \" + s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the inverted index is changing because now we work with the title too, and i use smooth because as we can see during the scraping in the title always was the word \"Wikipedia\" and if we add the title to the plot and intro we can understand that with the old idf formula we would get the idf for word title = 0 but now we will have at least log(2). So if someone will want to find the movie by the word \"Wikipedia\" there still will have top movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_ind2 = {}\n",
    "# doc_lenght = {}\n",
    "# for key,value in Inv_in.items():\n",
    "#     inv_ind2[key] = []\n",
    "\n",
    "\n",
    "# for path in movies_tsv:\n",
    "#     doc_num = int(re.findall(r'\\d+',re.findall(r'article_.+',path)[0])[0])\n",
    "#     df = pd.read_csv(path,sep = \"\\t\", index_col = False,encoding='utf-8')[[\"title\",\"clean_intro\",\"clean_plot\"]] # open the file with DataFrame\n",
    "#     text = prep_ti_text(df) # prepare the text with title, plot, intro\n",
    "#     words = split_text(text)\n",
    "#     terms = word_to_term(words , vocabulary)\n",
    "#     doc_lenght[doc_num] = count_length(terms,term_idf,len(terms))\n",
    "#     d_length = len(terms)\n",
    "#     unic_terms = list(set(terms))\n",
    "\n",
    "#     for t in unic_terms:    \n",
    "#         freq = terms.count(t)\n",
    "#         td_idf = freq * term_idf[t] # idf of term multiply on td\n",
    "#         inv_ind2[t].append((doc_num, td_idf/d_length))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Inverted_index_3.json', 'w') as fp:\n",
    "#     json.dump(inv_ind2, fp)\n",
    "    \n",
    "# with open('doc_lenght_3.json', 'w') as fp:\n",
    "#     json.dump(doc_lenght, fp)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the inverted index 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind3 = json.load(open('Inverted_index_3.json')) # as we can see now all key are str\n",
    "inv_ind3 = jsonKeys2int(inv_ind2) # this finction solved the problem with string key\n",
    "for key in inv_ind2.keys(): # after uploading json all tuples became lists \n",
    "    for i in range(len(inv_ind2[key])):\n",
    "        inv_ind2[key][i] = tuple(inv_ind2[key][i]) # turn back from lists to tuples\n",
    "for key in inv_ind2.keys():\n",
    "    for i in range(len(inv_ind2[key])):\n",
    "        inv_ind2[key][i] = tuple(inv_ind2[key][i])\n",
    "doc_lenght = json.load(open('doc_lenght_3.json')) # as we can see now all key are str\n",
    "doc_lenght = jsonKeys2int(doc_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use the function for second search engine but with different idf the results will be different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star future space\n"
     ]
    }
   ],
   "source": [
    "query = list(query_upload()) #take the string and create the words\n",
    "t_query = term_query(query,vocabulary)\n",
    "paths_doc = find_all_docs_consist_all_quiery(t_query,Inv_in) # i use first search engine to find all documents which consist all quaery\n",
    "if len(paths_doc) == 0: # if there is not documents which contains all words from the query\n",
    "    cossim = Search_engine_2_all_docs(query,vocabulary,term_idf,inv_ind3) # this function will count the cossine similarity for all documents which consists at least one wod from the query\n",
    "else:\n",
    "    cossim = Search_engine_2(query,vocabulary,term_idf,inv_ind3,paths_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see the cosine similarity has changed comparing with the old score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Jetsons Meet the Flintstones</td>\n",
       "      <td>The Jetsons Meet the Flintstones is a 1987 ani...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Jetsons_Meet...</td>\n",
       "      <td>0.578095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost in Space (film)</td>\n",
       "      <td>Lost in Space is a 1998 American science-fict...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lost_in_Space_(f...</td>\n",
       "      <td>0.302463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zathura: A Space Adventure</td>\n",
       "      <td>Zathura: A Space Adventure (also known simply...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zathura_(film)</td>\n",
       "      <td>0.263370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Phantom Planet</td>\n",
       "      <td>The Phantom Planet is a 1961 independently mad...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Phantom_Planet</td>\n",
       "      <td>0.250881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predestination (film)</td>\n",
       "      <td>Predestination is a 2014 Australian science f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Predestination_(...</td>\n",
       "      <td>0.192607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0  The Jetsons Meet the Flintstones   \n",
       "1              Lost in Space (film)   \n",
       "2        Zathura: A Space Adventure   \n",
       "3                The Phantom Planet   \n",
       "4             Predestination (film)   \n",
       "\n",
       "                                               intro  \\\n",
       "0  The Jetsons Meet the Flintstones is a 1987 ani...   \n",
       "1   Lost in Space is a 1998 American science-fict...   \n",
       "2   Zathura: A Space Adventure (also known simply...   \n",
       "3  The Phantom Planet is a 1961 independently mad...   \n",
       "4   Predestination is a 2014 Australian science f...   \n",
       "\n",
       "                                                 url  Similarity  \n",
       "0  https://en.wikipedia.org/wiki/The_Jetsons_Meet...    0.578095  \n",
       "1  https://en.wikipedia.org/wiki/Lost_in_Space_(f...    0.302463  \n",
       "2       https://en.wikipedia.org/wiki/Zathura_(film)    0.263370  \n",
       "3   https://en.wikipedia.org/wiki/The_Phantom_Planet    0.250881  \n",
       "4  https://en.wikipedia.org/wiki/Predestination_(...    0.192607  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "top_k(k,cossim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4th question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxLengthPalindrome(S):\n",
    "    combosList  = list(itertools.chain.from_iterable([list(itertools.combinations(S,i)) for i in range(1,len(S))])) #list form\n",
    "    combosString = [''.join(c) for c in combosList]  #string form of every possibile subsequence \n",
    "    chk=0\n",
    "    longest=0\n",
    "    for sub in combosString:     #for every subsequence\n",
    "            for k in range(len(sub)):   \n",
    "                if(sub[k]==sub[len(sub)-1-k]): #we test if first element is equal to last element(second with second-last and so on)\n",
    "                    chk+=1\n",
    "                    if(chk==len(sub)):  #if true we found a palindrome\n",
    "                        if(len(sub)>longest): \n",
    "                            longest=len(sub)   #we save the longest\n",
    "                else:\n",
    "                    chk=0\n",
    "                    break\n",
    "    return print('The longest Palindrome subsequence is %s ' %longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest Palindrome subsequence is 7 \n"
     ]
    }
   ],
   "source": [
    "MaxLengthPalindrome('DATAMININGSAPIENZA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
